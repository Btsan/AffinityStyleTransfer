{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdaIN_Affinity_StyleTransfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMaUOHwuyu/h01fSYUWWPDO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Btsan/AffinityStyleTransfer/blob/main/AdaIN_Affinity_StyleTransfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx73pdhJHmCk"
      },
      "source": [
        "This notebook was made in google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Hfi63qF7VM"
      },
      "source": [
        "datasets and the affinity estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLOIGaWvftDt",
        "outputId": "3aea4f91-c602-4220-a0e7-8efa9e0e0639"
      },
      "source": [
        "%%bash\r\n",
        "pip install av\r\n",
        "pip install --upgrade Pillow\r\n",
        "\r\n",
        "git clone https://github.com/xiaolonw/UVC-1.git\r\n",
        "cp -r UVC-1/* .\r\n",
        "\r\n",
        "wget -q https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-Unsupervised-trainval-480p.zip\r\n",
        "mkdir datasets\r\n",
        "unzip -q DAVIS-2017-Unsupervised-trainval-480p.zip -d datasets\r\n",
        "\r\n",
        "mkdir /root/.kaggle\r\n",
        "cp kaggle.json /root/.kaggle/\r\n",
        "chmod 600 ~/.kaggle/kaggle.json\r\n",
        "kaggle competitions download -c painter-by-numbers -f train_1.zip\r\n",
        "unzip -q train_1.zip -d datasets\r\n",
        "\r\n",
        "mkdir saves\r\n",
        "mkdir videos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting av\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/62/9a992be76f8e13ce0e3a24a838191b546805545116f9fc869bd11bd21b5f/av-8.0.2-cp36-cp36m-manylinux2010_x86_64.whl (36.9MB)\n",
            "Installing collected packages: av\n",
            "Successfully installed av-8.0.2\n",
            "Collecting Pillow\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/19/d4c25111d36163698396f93c363114cf1cddbacb24744f6612f25b6aa3d0/Pillow-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-8.0.1\n",
            "Downloading train_1.zip to /content\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\n",
            "Cloning into 'UVC-1'...\n",
            "ls: cannot access '/root/.kaggle/': No such file or directory\n",
            "\r  0%|          | 0.00/4.76G [00:00<?, ?B/s]\r  0%|          | 5.00M/4.76G [00:00<05:20, 15.9MB/s]\r  0%|          | 9.00M/4.76G [00:00<05:21, 15.9MB/s]\r  1%|          | 39.0M/4.76G [00:00<03:48, 22.2MB/s]\r  1%|1         | 49.0M/4.76G [00:00<03:01, 27.9MB/s]\r  1%|1         | 61.0M/4.76G [00:00<02:18, 36.4MB/s]\r  1%|1         | 73.0M/4.76G [00:01<01:52, 44.6MB/s]\r  2%|1         | 83.0M/4.76G [00:01<02:04, 40.2MB/s]\r  2%|2         | 109M/4.76G [00:01<01:32, 53.9MB/s] \r  3%|2         | 122M/4.76G [00:01<01:32, 54.0MB/s]\r  3%|2         | 133M/4.76G [00:02<02:36, 31.7MB/s]\r  3%|3         | 159M/4.76G [00:02<01:54, 43.1MB/s]\r  4%|3         | 173M/4.76G [00:02<01:37, 50.7MB/s]\r  4%|3         | 193M/4.76G [00:03<01:27, 56.3MB/s]\r  5%|4         | 222M/4.76G [00:03<01:05, 74.4MB/s]\r  5%|4         | 238M/4.76G [00:03<01:00, 80.4MB/s]\r  5%|5         | 257M/4.76G [00:03<01:00, 79.5MB/s]\r  6%|5         | 275M/4.76G [00:03<00:50, 96.1MB/s]\r  6%|5         | 289M/4.76G [00:03<00:50, 95.2MB/s]\r  6%|6         | 302M/4.76G [00:04<01:09, 69.0MB/s]\r  6%|6         | 314M/4.76G [00:04<01:00, 79.4MB/s]\r  7%|6         | 329M/4.76G [00:04<01:01, 77.3MB/s]\r  7%|7         | 343M/4.76G [00:04<00:52, 89.8MB/s]\r  7%|7         | 361M/4.76G [00:04<00:49, 96.2MB/s]\r  8%|7         | 372M/4.76G [00:04<00:51, 91.0MB/s]\r  8%|8         | 392M/4.76G [00:04<00:42, 110MB/s] \r  8%|8         | 405M/4.76G [00:05<00:45, 103MB/s]\r  9%|8         | 425M/4.76G [00:05<00:55, 83.3MB/s]\r  9%|9         | 454M/4.76G [00:05<00:43, 106MB/s] \r 10%|9         | 470M/4.76G [00:05<00:43, 106MB/s]\r 10%|9         | 484M/4.76G [00:05<00:50, 90.8MB/s]\r 10%|#         | 505M/4.76G [00:06<00:53, 85.6MB/s]\r 11%|#         | 516M/4.76G [00:06<00:53, 85.0MB/s]\r 11%|#1        | 541M/4.76G [00:06<00:42, 106MB/s] \r 11%|#1        | 556M/4.76G [00:06<01:00, 74.8MB/s]\r 12%|#1        | 577M/4.76G [00:07<01:13, 61.6MB/s]\r 12%|#2        | 603M/4.76G [00:07<00:55, 80.2MB/s]\r 13%|#2        | 617M/4.76G [00:07<01:12, 61.3MB/s]\r 13%|#3        | 641M/4.76G [00:08<01:03, 69.3MB/s]\r 14%|#3        | 669M/4.76G [00:08<00:49, 89.8MB/s]\r 14%|#4        | 685M/4.76G [00:08<00:49, 88.1MB/s]\r 14%|#4        | 705M/4.76G [00:08<00:43, 99.3MB/s]\r 15%|#4        | 719M/4.76G [00:08<00:41, 105MB/s] \r 15%|#5        | 732M/4.76G [00:08<00:42, 103MB/s]\r 15%|#5        | 745M/4.76G [00:09<00:53, 81.6MB/s]\r 16%|#5        | 770M/4.76G [00:09<00:41, 103MB/s] \r 16%|#6        | 785M/4.76G [00:09<00:49, 86.9MB/s]\r 17%|#6        | 809M/4.76G [00:09<00:49, 86.6MB/s]\r 17%|#7        | 829M/4.76G [00:09<00:40, 105MB/s] \r 17%|#7        | 843M/4.76G [00:09<00:43, 97.4MB/s]\r 18%|#7        | 865M/4.76G [00:10<00:39, 105MB/s] \r 18%|#8        | 890M/4.76G [00:10<00:32, 128MB/s]\r 19%|#8        | 906M/4.76G [00:10<00:57, 72.0MB/s]\r 19%|#9        | 937M/4.76G [00:10<00:48, 85.1MB/s]\r 20%|#9        | 953M/4.76G [00:11<00:44, 93.3MB/s]\r 20%|#9        | 969M/4.76G [00:11<00:38, 106MB/s] \r 20%|##        | 983M/4.76G [00:11<00:38, 106MB/s]\r 20%|##        | 996M/4.76G [00:11<00:57, 71.3MB/s]\r 21%|##        | 1.00G/4.76G [00:11<00:44, 91.0MB/s]\r 21%|##1       | 1.01G/4.76G [00:11<00:44, 89.7MB/s]\r 22%|##1       | 1.03G/4.76G [00:12<00:43, 91.9MB/s]\r 22%|##1       | 1.04G/4.76G [00:12<00:48, 82.8MB/s]\r 22%|##2       | 1.07G/4.76G [00:12<00:37, 105MB/s] \r 23%|##2       | 1.08G/4.76G [00:12<00:54, 72.9MB/s]\r 23%|##3       | 1.10G/4.76G [00:13<00:51, 76.1MB/s]\r 24%|##3       | 1.12G/4.76G [00:13<01:37, 40.3MB/s]\r 24%|##3       | 1.14G/4.76G [00:13<01:13, 53.2MB/s]\r 24%|##4       | 1.15G/4.76G [00:14<01:13, 52.6MB/s]\r 25%|##4       | 1.17G/4.76G [00:14<00:56, 67.7MB/s]\r 25%|##4       | 1.18G/4.76G [00:14<01:08, 55.9MB/s]\r 25%|##5       | 1.21G/4.76G [00:14<00:51, 73.7MB/s]\r 26%|##5       | 1.22G/4.76G [00:15<01:03, 59.8MB/s]\r 26%|##6       | 1.24G/4.76G [00:15<01:02, 60.0MB/s]\r 27%|##6       | 1.26G/4.76G [00:15<00:49, 75.9MB/s]\r 27%|##6       | 1.28G/4.76G [00:15<00:44, 84.3MB/s]\r 27%|##7       | 1.29G/4.76G [00:15<00:36, 102MB/s] \r 28%|##7       | 1.31G/4.76G [00:16<00:40, 91.9MB/s]\r 28%|##7       | 1.33G/4.76G [00:16<00:33, 109MB/s] \r 28%|##8       | 1.34G/4.76G [00:16<00:40, 89.8MB/s]\r 28%|##8       | 1.35G/4.76G [00:16<00:40, 90.0MB/s]\r 29%|##8       | 1.37G/4.76G [00:16<00:33, 110MB/s] \r 29%|##9       | 1.39G/4.76G [00:16<00:44, 81.5MB/s]\r 30%|##9       | 1.41G/4.76G [00:17<00:52, 68.5MB/s]\r 30%|##9       | 1.42G/4.76G [00:17<00:44, 81.3MB/s]\r 30%|###       | 1.44G/4.76G [00:17<00:37, 96.0MB/s]\r 30%|###       | 1.45G/4.76G [00:18<01:41, 35.0MB/s]\r 31%|###1      | 1.48G/4.76G [00:18<01:19, 44.1MB/s]\r 31%|###1      | 1.49G/4.76G [00:19<01:08, 51.0MB/s]\r 32%|###1      | 1.51G/4.76G [00:19<00:58, 59.2MB/s]\r 32%|###1      | 1.52G/4.76G [00:19<01:02, 55.4MB/s]\r 32%|###2      | 1.53G/4.76G [00:19<00:52, 65.6MB/s]\r 33%|###2      | 1.55G/4.76G [00:19<00:48, 70.8MB/s]\r 33%|###2      | 1.56G/4.76G [00:19<00:50, 67.3MB/s]\r 33%|###3      | 1.58G/4.76G [00:20<00:41, 81.5MB/s]\r 33%|###3      | 1.59G/4.76G [00:20<00:40, 83.8MB/s]\r 34%|###3      | 1.60G/4.76G [00:20<00:39, 86.5MB/s]\r 34%|###3      | 1.61G/4.76G [00:20<00:39, 86.5MB/s]\r 34%|###4      | 1.62G/4.76G [00:21<01:28, 37.9MB/s]\r 35%|###4      | 1.64G/4.76G [00:21<01:08, 48.6MB/s]\r 35%|###4      | 1.65G/4.76G [00:21<01:05, 51.2MB/s]\r 35%|###5      | 1.67G/4.76G [00:21<00:49, 66.7MB/s]\r 35%|###5      | 1.68G/4.76G [00:21<00:46, 71.6MB/s]\r 36%|###5      | 1.70G/4.76G [00:21<00:38, 85.6MB/s]\r 36%|###5      | 1.71G/4.76G [00:21<00:40, 80.4MB/s]\r 36%|###6      | 1.74G/4.76G [00:22<00:32, 101MB/s] \r 37%|###6      | 1.75G/4.76G [00:22<00:38, 84.8MB/s]\r 37%|###7      | 1.77G/4.76G [00:22<00:34, 93.7MB/s]\r 38%|###7      | 1.79G/4.76G [00:22<00:28, 113MB/s] \r 38%|###8      | 1.81G/4.76G [00:22<00:26, 119MB/s]\r 38%|###8      | 1.82G/4.76G [00:22<00:25, 122MB/s]\r 39%|###8      | 1.84G/4.76G [00:23<00:42, 73.0MB/s]\r 39%|###8      | 1.85G/4.76G [00:23<00:46, 67.2MB/s]\r 39%|###9      | 1.87G/4.76G [00:23<00:40, 76.5MB/s]\r 40%|###9      | 1.89G/4.76G [00:23<00:33, 93.0MB/s]\r 40%|###9      | 1.90G/4.76G [00:24<00:36, 83.1MB/s]\r 40%|####      | 1.92G/4.76G [00:24<00:30, 101MB/s] \r 41%|####      | 1.94G/4.76G [00:24<00:30, 99.3MB/s]\r 41%|####      | 1.95G/4.76G [00:25<01:20, 37.5MB/s]\r 41%|####1     | 1.97G/4.76G [00:25<00:59, 50.3MB/s]\r 42%|####1     | 1.98G/4.76G [00:25<00:50, 59.2MB/s]\r 42%|####1     | 2.00G/4.76G [00:25<00:43, 68.6MB/s]\r 42%|####2     | 2.01G/4.76G [00:25<00:36, 80.5MB/s]\r 42%|####2     | 2.02G/4.76G [00:25<00:35, 83.6MB/s]\r 43%|####2     | 2.03G/4.76G [00:25<00:31, 93.6MB/s]\r 43%|####3     | 2.05G/4.76G [00:26<00:35, 82.3MB/s]\r 43%|####3     | 2.06G/4.76G [00:26<01:38, 29.3MB/s]\r 44%|####3     | 2.09G/4.76G [00:27<01:16, 37.6MB/s]\r 44%|####4     | 2.11G/4.76G [00:27<00:55, 50.8MB/s]\r 45%|####4     | 2.13G/4.76G [00:27<00:55, 50.6MB/s]\r 45%|####4     | 2.14G/4.76G [00:28<01:06, 42.1MB/s]\r 45%|####5     | 2.16G/4.76G [00:28<00:50, 55.7MB/s]\r 46%|####5     | 2.17G/4.76G [00:28<00:48, 57.1MB/s]\r 46%|####6     | 2.20G/4.76G [00:28<00:38, 71.8MB/s]\r 46%|####6     | 2.21G/4.76G [00:28<00:42, 64.9MB/s]\r 47%|####6     | 2.24G/4.76G [00:29<00:37, 72.5MB/s]\r 47%|####7     | 2.25G/4.76G [00:29<01:21, 33.1MB/s]\r 48%|####7     | 2.27G/4.76G [00:30<01:02, 42.9MB/s]\r 48%|####8     | 2.29G/4.76G [00:30<00:48, 54.6MB/s]\r 48%|####8     | 2.31G/4.76G [00:30<00:39, 66.2MB/s]\r 49%|####8     | 2.32G/4.76G [00:30<00:37, 70.4MB/s]\r 49%|####8     | 2.33G/4.76G [00:30<00:36, 70.8MB/s]\r 49%|####9     | 2.34G/4.76G [00:30<00:38, 67.2MB/s]\r 50%|####9     | 2.37G/4.76G [00:30<00:29, 87.1MB/s]\r 50%|#####     | 2.38G/4.76G [00:31<00:33, 77.1MB/s]\r 50%|#####     | 2.40G/4.76G [00:31<00:28, 90.1MB/s]\r 51%|#####     | 2.41G/4.76G [00:31<00:34, 72.1MB/s]\r 51%|#####1    | 2.43G/4.76G [00:31<00:30, 82.8MB/s]\r 51%|#####1    | 2.44G/4.76G [00:32<00:40, 61.9MB/s]\r 52%|#####1    | 2.46G/4.76G [00:32<00:31, 78.5MB/s]\r 52%|#####1    | 2.47G/4.76G [00:32<00:33, 74.2MB/s]\r 52%|#####2    | 2.49G/4.76G [00:32<00:33, 72.9MB/s]\r 53%|#####2    | 2.51G/4.76G [00:32<00:26, 92.0MB/s]\r 53%|#####3    | 2.52G/4.76G [00:32<00:26, 92.1MB/s]\r 53%|#####3    | 2.53G/4.76G [00:33<00:34, 69.6MB/s]\r 54%|#####3    | 2.55G/4.76G [00:33<00:28, 84.1MB/s]\r 54%|#####3    | 2.56G/4.76G [00:33<00:29, 79.1MB/s]\r 54%|#####4    | 2.58G/4.76G [00:33<00:27, 84.5MB/s]\r 55%|#####4    | 2.59G/4.76G [00:33<00:28, 80.6MB/s]\r 55%|#####5    | 2.62G/4.76G [00:33<00:25, 89.2MB/s]\r 55%|#####5    | 2.63G/4.76G [00:34<00:32, 70.3MB/s]\r 56%|#####5    | 2.65G/4.76G [00:34<00:29, 75.8MB/s]\r 56%|#####5    | 2.66G/4.76G [00:34<00:41, 55.0MB/s]\r 56%|#####6    | 2.68G/4.76G [00:34<00:31, 70.9MB/s]\r 57%|#####6    | 2.69G/4.76G [00:35<00:32, 69.2MB/s]\r 57%|#####6    | 2.70G/4.76G [00:35<00:27, 81.6MB/s]\r 57%|#####7    | 2.72G/4.76G [00:35<00:24, 88.5MB/s]\r 57%|#####7    | 2.73G/4.76G [00:35<00:28, 76.4MB/s]\r 58%|#####7    | 2.74G/4.76G [00:35<00:24, 87.5MB/s]\r 58%|#####7    | 2.76G/4.76G [00:35<00:24, 89.0MB/s]\r 58%|#####8    | 2.77G/4.76G [00:35<00:21, 97.5MB/s]\r 58%|#####8    | 2.78G/4.76G [00:36<00:30, 68.7MB/s]\r 59%|#####8    | 2.81G/4.76G [00:36<00:27, 75.9MB/s]\r 59%|#####9    | 2.82G/4.76G [00:36<00:23, 88.2MB/s]\r 60%|#####9    | 2.84G/4.76G [00:36<00:22, 91.9MB/s]\r 60%|#####9    | 2.85G/4.76G [00:36<00:19, 107MB/s] \r 60%|######    | 2.87G/4.76G [00:36<00:19, 103MB/s]\r 61%|######    | 2.89G/4.76G [00:37<00:18, 111MB/s]\r 61%|######1   | 2.90G/4.76G [00:38<01:06, 30.1MB/s]\r 62%|######1   | 2.93G/4.76G [00:38<00:54, 35.8MB/s]\r 62%|######1   | 2.95G/4.76G [00:38<00:41, 46.5MB/s]\r 62%|######2   | 2.96G/4.76G [00:38<00:33, 57.1MB/s]\r 62%|######2   | 2.97G/4.76G [00:39<00:28, 67.2MB/s]\r 63%|######2   | 2.99G/4.76G [00:39<00:24, 76.7MB/s]\r 63%|######3   | 3.00G/4.76G [00:39<00:25, 73.7MB/s]\r 63%|######3   | 3.01G/4.76G [00:39<00:26, 71.8MB/s]\r 64%|######3   | 3.03G/4.76G [00:39<00:20, 91.4MB/s]\r 64%|######4   | 3.05G/4.76G [00:39<00:22, 80.0MB/s]\r 65%|######4   | 3.07G/4.76G [00:40<00:18, 99.7MB/s]\r 65%|######4   | 3.08G/4.76G [00:40<00:21, 84.6MB/s]\r 65%|######5   | 3.10G/4.76G [00:41<00:39, 45.1MB/s]\r 66%|######5   | 3.13G/4.76G [00:41<00:29, 59.9MB/s]\r 66%|######6   | 3.14G/4.76G [00:41<00:24, 71.9MB/s]\r 66%|######6   | 3.15G/4.76G [00:41<00:28, 59.5MB/s]\r 67%|######6   | 3.18G/4.76G [00:42<00:30, 55.5MB/s]\r 67%|######7   | 3.21G/4.76G [00:42<00:22, 73.6MB/s]\r 68%|######7   | 3.22G/4.76G [00:42<00:25, 64.3MB/s]\r 68%|######8   | 3.24G/4.76G [00:42<00:21, 76.3MB/s]\r 68%|######8   | 3.25G/4.76G [00:43<00:20, 77.1MB/s]\r 69%|######8   | 3.27G/4.76G [00:43<00:16, 96.0MB/s]\r 69%|######9   | 3.29G/4.76G [00:43<00:30, 51.2MB/s]\r 70%|######9   | 3.31G/4.76G [00:43<00:24, 62.4MB/s]\r 70%|######9   | 3.33G/4.76G [00:44<00:22, 69.3MB/s]\r 70%|#######   | 3.34G/4.76G [00:44<00:17, 85.8MB/s]\r 71%|#######   | 3.36G/4.76G [00:44<00:18, 83.0MB/s]\r 71%|#######   | 3.37G/4.76G [00:44<00:16, 92.2MB/s]\r 71%|#######1  | 3.38G/4.76G [00:44<00:22, 65.4MB/s]\r 71%|#######1  | 3.40G/4.76G [00:44<00:17, 82.9MB/s]\r 72%|#######1  | 3.41G/4.76G [00:45<00:17, 80.5MB/s]\r 72%|#######2  | 3.44G/4.76G [00:45<00:16, 88.3MB/s]\r 73%|#######2  | 3.46G/4.76G [00:45<00:12, 111MB/s] \r 73%|#######3  | 3.48G/4.76G [00:45<00:13, 102MB/s]\r 74%|#######3  | 3.50G/4.76G [00:45<00:13, 97.5MB/s]\r 74%|#######4  | 3.52G/4.76G [00:46<00:14, 92.4MB/s]\r 74%|#######4  | 3.54G/4.76G [00:46<00:11, 110MB/s] \r 75%|#######4  | 3.56G/4.76G [00:46<00:12, 99.2MB/s]\r 75%|#######5  | 3.58G/4.76G [00:46<00:10, 122MB/s] \r 76%|#######5  | 3.60G/4.76G [00:46<00:13, 95.0MB/s]\r 76%|#######5  | 3.61G/4.76G [00:47<00:17, 71.3MB/s]\r 76%|#######6  | 3.63G/4.76G [00:47<00:21, 55.7MB/s]\r 77%|#######6  | 3.64G/4.76G [00:47<00:19, 62.9MB/s]\r 77%|#######6  | 3.66G/4.76G [00:47<00:16, 70.8MB/s]\r 77%|#######7  | 3.67G/4.76G [00:48<00:14, 81.8MB/s]\r 78%|#######7  | 3.69G/4.76G [00:48<00:11, 96.1MB/s]\r 78%|#######7  | 3.70G/4.76G [00:48<00:12, 88.9MB/s]\r 78%|#######8  | 3.73G/4.76G [00:48<00:12, 91.6MB/s]\r 79%|#######8  | 3.74G/4.76G [00:48<00:16, 67.8MB/s]\r 79%|#######8  | 3.75G/4.76G [00:49<00:12, 82.9MB/s]\r 79%|#######9  | 3.77G/4.76G [00:49<00:11, 88.8MB/s]\r 80%|#######9  | 3.78G/4.76G [00:49<00:15, 68.9MB/s]\r 80%|#######9  | 3.80G/4.76G [00:49<00:15, 66.8MB/s]\r 80%|########  | 3.81G/4.76G [00:49<00:13, 75.8MB/s]\r 80%|########  | 3.83G/4.76G [00:50<00:11, 89.5MB/s]\r 81%|########  | 3.84G/4.76G [00:50<00:11, 86.8MB/s]\r 81%|########1 | 3.86G/4.76G [00:50<00:09, 105MB/s] \r 81%|########1 | 3.88G/4.76G [00:50<00:10, 94.2MB/s]\r 82%|########1 | 3.89G/4.76G [00:50<00:09, 94.5MB/s]\r 82%|########1 | 3.90G/4.76G [00:50<00:09, 96.6MB/s]\r 82%|########2 | 3.92G/4.76G [00:50<00:07, 117MB/s] \r 83%|########2 | 3.94G/4.76G [00:51<00:08, 105MB/s]\r 83%|########3 | 3.95G/4.76G [00:51<00:07, 117MB/s]\r 83%|########3 | 3.96G/4.76G [00:51<00:07, 108MB/s]\r 84%|########3 | 3.99G/4.76G [00:51<00:08, 97.1MB/s]\r 84%|########4 | 4.02G/4.76G [00:51<00:07, 108MB/s] \r 85%|########4 | 4.04G/4.76G [00:51<00:05, 129MB/s]\r 85%|########5 | 4.05G/4.76G [00:52<00:07, 104MB/s]\r 86%|########5 | 4.07G/4.76G [00:52<00:06, 118MB/s]\r 86%|########5 | 4.08G/4.76G [00:52<00:06, 120MB/s]\r 86%|########6 | 4.10G/4.76G [00:52<00:05, 139MB/s]\r 87%|########6 | 4.12G/4.76G [00:52<00:05, 126MB/s]\r 87%|########7 | 4.14G/4.76G [00:52<00:05, 131MB/s]\r 87%|########7 | 4.16G/4.76G [00:52<00:05, 114MB/s]\r 88%|########7 | 4.17G/4.76G [00:53<00:05, 113MB/s]\r 88%|########7 | 4.18G/4.76G [00:53<00:05, 120MB/s]\r 88%|########8 | 4.19G/4.76G [00:53<00:06, 95.7MB/s]\r 89%|########8 | 4.21G/4.76G [00:53<00:05, 109MB/s] \r 89%|########8 | 4.22G/4.76G [00:53<00:05, 110MB/s]\r 89%|########9 | 4.24G/4.76G [00:53<00:06, 90.2MB/s]\r 89%|########9 | 4.25G/4.76G [00:53<00:05, 107MB/s] \r 90%|########9 | 4.27G/4.76G [00:54<00:05, 96.0MB/s]\r 90%|######### | 4.29G/4.76G [00:54<00:04, 101MB/s] \r 91%|######### | 4.31G/4.76G [00:54<00:04, 120MB/s]\r 91%|######### | 4.32G/4.76G [00:54<00:03, 124MB/s]\r 91%|#########1| 4.34G/4.76G [00:54<00:03, 140MB/s]\r 92%|#########1| 4.36G/4.76G [00:54<00:03, 142MB/s]\r 92%|#########1| 4.37G/4.76G [00:55<00:04, 98.3MB/s]\r 92%|#########2| 4.39G/4.76G [00:55<00:03, 118MB/s] \r 93%|#########2| 4.41G/4.76G [00:55<00:03, 113MB/s]\r 93%|#########3| 4.43G/4.76G [00:55<00:02, 128MB/s]\r 93%|#########3| 4.44G/4.76G [00:55<00:02, 123MB/s]\r 94%|#########3| 4.46G/4.76G [00:55<00:02, 110MB/s]\r 94%|#########3| 4.47G/4.76G [00:55<00:02, 119MB/s]\r 94%|#########4| 4.48G/4.76G [00:55<00:02, 129MB/s]\r 95%|#########4| 4.50G/4.76G [00:56<00:02, 115MB/s]\r 95%|#########4| 4.51G/4.76G [00:56<00:02, 108MB/s]\r 95%|#########5| 4.54G/4.76G [00:56<00:01, 133MB/s]\r 96%|#########5| 4.55G/4.76G [00:56<00:01, 110MB/s]\r 96%|#########6| 4.57G/4.76G [00:56<00:01, 112MB/s]\r 97%|#########6| 4.59G/4.76G [00:56<00:01, 132MB/s]\r 97%|#########6| 4.61G/4.76G [00:56<00:01, 123MB/s]\r 97%|#########7| 4.62G/4.76G [00:57<00:01, 123MB/s]\r 97%|#########7| 4.64G/4.76G [00:57<00:01, 115MB/s]\r 98%|#########7| 4.66G/4.76G [00:57<00:00, 132MB/s]\r 98%|#########8| 4.67G/4.76G [00:57<00:00, 139MB/s]\r 99%|#########8| 4.69G/4.76G [00:57<00:00, 143MB/s]\r 99%|#########8| 4.70G/4.76G [00:57<00:00, 147MB/s]\r 99%|#########9| 4.72G/4.76G [00:57<00:00, 144MB/s]\r100%|#########9| 4.74G/4.76G [00:57<00:00, 158MB/s]\r100%|##########| 4.76G/4.76G [00:58<00:00, 87.9MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyBk96rzGAbw"
      },
      "source": [
        "vgg modified with one extra 1x1 convolutional layer and reflection padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUUEvgIjZB68"
      },
      "source": [
        "%%bash\r\n",
        "wget --load-cookies /tmp/cookies.txt \\\r\n",
        "\"https://docs.google.com/uc?export=download&confirm=\\\r\n",
        "$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1zo2t62c26xaHhMBDyVcvjCHDNFOkalZs' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1zo2t62c26xaHhMBDyVcvjCHDNFOkalZs\" \\\r\n",
        "-O pretrained.zip && rm -rf /tmp/cookies.txt\r\n",
        "unzip -q pretrained.zip -d saves"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbGdnMHpGNmC"
      },
      "source": [
        "dataset classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmRxgqnrYKi1"
      },
      "source": [
        "import torch\r\n",
        "import torch.utils.data as data\r\n",
        "from torchvision import transforms\r\n",
        "from torchvision import io\r\n",
        "\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "class ImageFolderDataset(data.Dataset):\r\n",
        "\tdef __init__(self, folder, transform=None):\r\n",
        "\t\tsuper(ImageFolderDataset, self).__init__()\r\n",
        "\t\tself.folder = folder\r\n",
        "\t\tself.images = list(Path(self.folder).glob('**/*.*'))\r\n",
        "\t\tself.images.sort()\r\n",
        "\t\tself.transform = transform\r\n",
        "\r\n",
        "\t# i^th item from the dataset\r\n",
        "\tdef __getitem__(self, index):\r\n",
        "\t\tindex %= len(self.images)\r\n",
        "\t\timg = self.images[index]\r\n",
        "\t\ttry:\r\n",
        "\t\t\timg = io.read_image(str(img))\r\n",
        "\t\texcept Exception as e:\r\n",
        "\t\t\t# print(\"Corrupted: \" + str(self.images[index]))\r\n",
        "\t\t\tself.images.pop(index)\r\n",
        "\t\t\treturn self[index % len(self.images)]\r\n",
        "\t\tif self.transform is not None:\r\n",
        "\t\t\timg = self.transform(img)\r\n",
        "\t\tif img.shape[0] == 1:\r\n",
        "\t\t\timg = img.expand((3, -1, -1))\r\n",
        "\t\telif img.shape[0] == 4:\r\n",
        "\t\t\timg =  img[:3, :, :]\r\n",
        "\t\treturn img\r\n",
        "\r\n",
        "\tdef __len__(self):\r\n",
        "\t\treturn len(self.images)\r\n",
        "\r\n",
        "\tdef name(self):\r\n",
        "\t\treturn 'ImageFolderDataset'\r\n",
        "\r\n",
        "class DavisDataset(data.Dataset):\r\n",
        "    def __init__(self, img_folder, transform=None):\r\n",
        "        super(DavisDataset, self).__init__()\r\n",
        "        # root directory of folder\r\n",
        "        self.folder = img_folder\r\n",
        "        # search all files in subdirectories\r\n",
        "        self.sub_folders = list(Path(self.folder).glob('*/'))\r\n",
        "        self.frame_dict = {}\r\n",
        "        for f in self.sub_folders:\r\n",
        "            self.frame_dict[f] = list(Path(f).glob('**/*.*'))\r\n",
        "            self.frame_dict[f].sort()\r\n",
        "        self.lengths = [len(self.frame_dict[f]) - 20 for f in self.sub_folders]\r\n",
        "        self.total_length = 0\r\n",
        "        for l in self.lengths:\r\n",
        "            self.total_length += l\r\n",
        "        # transform function\r\n",
        "        self.transform = transform\r\n",
        "        self.batch_size=8\r\n",
        "        \r\n",
        "    # i^th item from the dataset\r\n",
        "    def __getitem__(self, index):\r\n",
        "        i = l = count = 0\r\n",
        "        for i, l in enumerate(self.lengths):\r\n",
        "            count += l\r\n",
        "            if count >= index:\r\n",
        "                break\r\n",
        "        folder = self.sub_folders[i]\r\n",
        "        sub_idx =  l - count + index - l # 30 - (120 - 100)\r\n",
        "        frame1 = self.frame_dict[folder][sub_idx]\r\n",
        "        frame2 = self.frame_dict[folder][sub_idx + 7] #1\r\n",
        "        frame3 = self.frame_dict[folder][sub_idx + 2] #2\r\n",
        "        frame4 = self.frame_dict[folder][sub_idx + 8] #3\r\n",
        "        frame5 = self.frame_dict[folder][sub_idx + 3] #4\r\n",
        "        frame6 = self.frame_dict[folder][sub_idx + 9]#10\r\n",
        "        frame7 = self.frame_dict[folder][sub_idx + 4]#15\r\n",
        "        frame8 = self.frame_dict[folder][sub_idx + 10]#20\r\n",
        "        frame1 = io.read_image(str(frame1))\r\n",
        "        frame2 = io.read_image(str(frame2))\r\n",
        "        frame3 = io.read_image(str(frame3))\r\n",
        "        frame4 = io.read_image(str(frame4))\r\n",
        "        frame5 = io.read_image(str(frame5))\r\n",
        "        frame6 = io.read_image(str(frame6))\r\n",
        "        frame7 = io.read_image(str(frame7))\r\n",
        "        frame8 = io.read_image(str(frame8))\r\n",
        "        frames = torch.stack([frame1, frame2, frame3, frame4, frame5, frame6, frame7, frame8])\r\n",
        "        if self.transform is not None:\r\n",
        "            frames = self.transform(frames)\r\n",
        "        return frames\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.total_length\r\n",
        "\r\n",
        "    def name(self):\r\n",
        "        return 'DavisDataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VjTcH41GQjx"
      },
      "source": [
        "AdaIN style classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CHC3YDBXsV_"
      },
      "source": [
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torch import optim\r\n",
        "import torchvision.models as models\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision.utils import save_image\r\n",
        "\r\n",
        "from time import time\r\n",
        "\r\n",
        "class VGGEncoder(nn.Module):\r\n",
        "\tdef __init__(self, path=None):\r\n",
        "\t\tsuper(VGGEncoder, self).__init__()\r\n",
        "\r\n",
        "\t\tif path is not None:\r\n",
        "\t\t\tvgg = vgg_normalised = nn.Sequential( # Sequential,\r\n",
        "\t\t\t\tnn.Conv2d(3,3,(1, 1)),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(3,64,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(), # 1 1\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(64,64,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(64,128,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(), # 2 1\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(128,128,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(128,256,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(), # 3 1\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(256,256,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(256,256,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(256,256,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(256,512,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(), # 4 1\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(512,512,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(512,512,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(512,512,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(512,512,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(512,512,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(512,512,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(512,512,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t)\r\n",
        "\t\t\tsave_path = Path(path)\r\n",
        "\t\t\tif save_path.is_file():\r\n",
        "\t\t\t\ttry:\r\n",
        "\t\t\t\t\tstate_dict = torch.load(str(save_path))\r\n",
        "\t\t\t\t\tvgg.load_state_dict(state_dict)\r\n",
        "\t\t\t\t\tprint(\"Loaded vgg\", save_path)\r\n",
        "\t\t\t\texcept:\r\n",
        "\t\t\t\t\tprint(\"Error: previous vgg failed to load\")\r\n",
        "\t\t\t\t\tpass\r\n",
        "\t\t\telse:\r\n",
        "\t\t\t\tvgg = models.vgg19(pretrained=True).features.eval()\r\n",
        "\t\t\t\tprint(\"Initializing new vgg\")\r\n",
        "\t\telse:\r\n",
        "\t\t\tvgg = models.vgg19(pretrained=True).features.eval()\r\n",
        "\t\tfeatures = list(vgg.children())\r\n",
        "\t\t#transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ?\r\n",
        "\t\tself.enc_1 = nn.Sequential(*features[0:4]) \r\n",
        "\t\tself.enc_2 = nn.Sequential(*features[4:11])\r\n",
        "\t\tself.enc_3 = nn.Sequential(*features[11:18])\r\n",
        "\t\tself.enc_4 = nn.Sequential(*features[18:31]) # content layer\r\n",
        "\t\tself.model = nn.Sequential(*features[0:30])\r\n",
        "\r\n",
        "\tdef forward(self, x):\r\n",
        "\t\tx = self.model(x)\r\n",
        "\t\tself.content = F.relu(x).detach() # save content map for loss\r\n",
        "\t\treturn x\r\n",
        "\r\n",
        "\tdef compute_loss(self, g, s):\r\n",
        "\t\tg1 = self.enc_1(g)\r\n",
        "\t\tg2 = self.enc_2(g1)\r\n",
        "\t\tg3 = self.enc_3(g2)\r\n",
        "\t\tg4 = self.enc_4(g3)\r\n",
        "\t\tcontent_loss = F.mse_loss(g4, self.content)\r\n",
        "\r\n",
        "\t\tif s.dim() == 3:\r\n",
        "\t\t\ts = s.unsqueeze(0)\r\n",
        "\r\n",
        "\t\ts1 = self.enc_1(s)\r\n",
        "\t\ts2 = self.enc_2(s1)\r\n",
        "\t\ts3 = self.enc_3(s2)\r\n",
        "\t\ts4 = self.enc_4(s3)\r\n",
        "\t\tstyle_loss = F.mse_loss(g1.mean((2, 3)), s1.mean((2, 3))) \\\r\n",
        "\t\t\t+ F.mse_loss(g2.mean((2, 3)), s2.mean((2, 3))) \\\r\n",
        "\t\t\t+ F.mse_loss(g3.mean((2, 3)), s3.mean((2, 3))) \\\r\n",
        "\t\t\t+ F.mse_loss(g4.mean((2, 3)), s4.mean((2, 3)))\r\n",
        "\t\tstyle_loss += F.mse_loss(g1.std((2, 3)), s1.std((2, 3))) \\\r\n",
        "\t\t\t+ F.mse_loss(g2.std((2, 3)), s2.std((2, 3))) \\\r\n",
        "\t\t\t+ F.mse_loss(g3.std((2, 3)), s3.std((2, 3))) \\\r\n",
        "\t\t\t+ F.mse_loss(g4.std((2, 3)), s4.std((2, 3)))\r\n",
        "\r\n",
        "\t\treturn content_loss, style_loss\r\n",
        "\r\n",
        "class AdaIN(nn.Module):\r\n",
        "\tdef __init__(self, encoder=None):\r\n",
        "\t\tsuper(AdaIN, self).__init__()\r\n",
        "\r\n",
        "\t\tif encoder is None:\r\n",
        "\t\t\tself.model = nn.Sequential(\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(512,256,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.UpsamplingNearest2d(scale_factor=2),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(256,256,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(256,256,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(256,256,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(256,128,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.UpsamplingNearest2d(scale_factor=2),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(128,128,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(128,64,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.UpsamplingNearest2d(scale_factor=2),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(64,64,(3, 3)),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.ReflectionPad2d((1, 1, 1, 1)),\r\n",
        "\t\t\t\tnn.Conv2d(64,3,(3, 3)))\r\n",
        "\t\telse:\r\n",
        "\t\t\tchildren = [] \r\n",
        "\t\t\tlayers = list(encoder.children())\r\n",
        "\t\t\tfor i in range(len(layers)-1, -1, -1):\r\n",
        "\t\t\t\tif isinstance(layers[i], nn.Conv2d):\r\n",
        "\t\t\t\t\tin_channels, out_channels = layers[i].weight.shape[:2]\r\n",
        "\t\t\t\t\tprint(\"conv_{}\".format(i), in_channels, out_channels)\r\n",
        "\t\t\t\t\tchildren.append(nn.Conv2d(in_channels, out_channels, 3, padding=1, padding_mode='reflect'))\r\n",
        "\t\t\t\t\tchildren.append(nn.ReLU())\r\n",
        "\t\t\t\telif isinstance(layers[i], nn.MaxPool2d):\r\n",
        "\t\t\t\t\tchildren.append(nn.Upsample(scale_factor=2, mode='nearest'))\r\n",
        "\t\t\tchildren.pop()\r\n",
        "\t\t\tprint(children)\r\n",
        "\t\t\tself.model = nn.Sequential(*children)\r\n",
        "\r\n",
        "\tdef forward(self, content, style):\r\n",
        "\t\ttarget_std = style.std((2, 3))\r\n",
        "\t\ttarget_mean = style.mean((2, 3))\r\n",
        "\t\tadain = [F.instance_norm(content[i].unsqueeze(0), weight=target_std[i], bias=target_mean[i]) for i  in range(len(content))]\r\n",
        "\t\tadain = torch.cat(adain)\r\n",
        "\t\tadain = F.relu(adain) # assumes last vgg layer was conv2d\r\n",
        "\t\ty = self.model(adain)\r\n",
        "\t\ty = torch.tanh(y)\r\n",
        "\t\treturn y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9ncqRr2GU5q"
      },
      "source": [
        "Affinity loss calculation class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHfuPB9GYJhv"
      },
      "source": [
        "from scipy import signal\r\n",
        "\r\n",
        "class AffinityLoss(nn.Module):\r\n",
        "    def __init__(self, affinity_model):\r\n",
        "        super(AffinityLoss, self).__init__()\r\n",
        "\r\n",
        "        self.gray_encoder = affinity_model.gray_encoder\r\n",
        "        self.nlm = affinity_model.nlm\r\n",
        "        self.softmax = affinity_model.softmax\r\n",
        "\r\n",
        "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n",
        "        self.mse = nn.MSELoss()\r\n",
        "\r\n",
        "        for p in self.gray_encoder.parameters():\r\n",
        "            p.requires_grad = False\r\n",
        "\r\n",
        "    def compute_affinity(self, frame1, frame2):\r\n",
        "        frame1 = self.normalize(frame1)\r\n",
        "        frame2 = self.normalize(frame2)\r\n",
        "\r\n",
        "        Fframe1 = self.gray_encoder(frame1)\r\n",
        "        Fframe2 = self.gray_encoder(frame2)\r\n",
        "\r\n",
        "        aff = self.nlm(Fframe1, Fframe2)\r\n",
        "        return aff\r\n",
        "\r\n",
        "    def forward(self, frame1, frame2, generated1, generated2, eye=0.):\r\n",
        "        aff = self.compute_affinity(frame1, frame2)\r\n",
        "        aff_norm_1 = self.softmax(aff * 10)\r\n",
        "        aff = self.compute_affinity(generated1, generated2)\r\n",
        "        aff_norm_2 = self.softmax(aff * 10)\r\n",
        "\r\n",
        "        n, h, w = aff_norm_1.shape\r\n",
        "        if eye > 0:\r\n",
        "            diag = eye * torch.eye(h, w).unsqueeze(0).expand_as(aff_norm_1).to(aff_norm_1.device)\r\n",
        "            aff_norm_1 += diag\r\n",
        "\r\n",
        "\r\n",
        "        loss = F.mse_loss(aff_norm_1, aff_norm_2, reduction='none')\r\n",
        "        gkern1d = torch.tensor(signal.gaussian(h, w), device=loss.device)\r\n",
        "        gkern2d = torch.outer(gkern1d, gkern1d).unsqueeze(0).expand_as(loss)\r\n",
        "        loss = (loss * gkern2d).mean()\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "def total_variation_loss(x):\r\n",
        "\tN, C, H, W = x.shape\r\n",
        "\ta = torch.square(x[:, :,: H - 1, : W - 1] - x[:, :, 1:, : W - 1])\r\n",
        "\tb = torch.square(x[:, :, : H - 1, : W - 1] - x[:, :, : H - 1, 1:])\r\n",
        "\treturn torch.sum(torch.pow(a + b, 1.25))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XTA0dH_GX5y"
      },
      "source": [
        "Load the affinity estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJY9IglWfKHo",
        "outputId": "61dd239c-d059-443b-edac-128b6ebd7251"
      },
      "source": [
        "from collections import OrderedDict\r\n",
        "from libs.model import Model_switchGTfixdot_swCC_Res as Affinity\r\n",
        "\r\n",
        "def get_affinity_loss(path=\"weights/checkpoint_latest.pth.tar\"):\r\n",
        "\taff_model = Affinity(pretrainRes=False, temp=1, uselayer=4)\r\n",
        "\tcheckpoint = torch.load(path)\r\n",
        "\r\n",
        "\tnew_state_dict = OrderedDict()\r\n",
        "\tfor k, v in checkpoint[\"state_dict\"].items():\r\n",
        "\t\tname = k[7:] # remove `module.`\r\n",
        "\t\tnew_state_dict[name] = v\r\n",
        "\r\n",
        "\taff_model.load_state_dict(new_state_dict)\r\n",
        "\tprint(\"loaded affinity model\")\r\n",
        "\r\n",
        "\taff_model = AffinityLoss(aff_model)\r\n",
        "\treturn aff_model\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "aff_loss = get_affinity_loss().to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded affinity model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIpHUawrGbua"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge8urO4DY-bW"
      },
      "source": [
        "def test(encoder, decoder, name=\"test.mp4\", imsize=256, video_path=\"motorbike\", style_path=\"style.jpg\"):\r\n",
        "    # resize = transforms.Resize((imsize, imsize))\r\n",
        "    dataset = ImageFolderDataset(\"datasets/DAVIS/JPEGImages/480p/\" + video_path) #,resize\r\n",
        "    dataloader = DataLoader(dataset, 4, shuffle=False)\r\n",
        "\r\n",
        "    pred = []\r\n",
        "    with torch.no_grad():\r\n",
        "        style_im = io.read_image(style_path).unsqueeze(0).to(device) / 255\r\n",
        "        style_ft = encoder(style_im).repeat(4, 1, 1, 1)\r\n",
        "        del style_im\r\n",
        "        for idx, im in enumerate(dataloader):\r\n",
        "            im = im.to(device) / 255\r\n",
        "            # im = im.unsqueeze(0).to(device) / 255\r\n",
        "            p = decoder(encoder(im), style_ft)\r\n",
        "            pred.append(p.cpu())\r\n",
        "\r\n",
        "        if len(pred[0].shape) == 4:\r\n",
        "                pred = torch.cat(pred, dim=0)\r\n",
        "        else:\r\n",
        "            pred = torch.stack(pred, dim=0)\r\n",
        "        pred = pred.permute(0, 2, 3, 1)\r\n",
        "        pred = (pred * 255 + 0.5).clamp(0, 255)\r\n",
        "        io.write_video(name, pred, 15)\r\n",
        "    return\r\n",
        "\r\n",
        "def train(encoder, decoder, optimizer, content_dataset, style_dataset, affinity_loss, imsize=256, epochs=1, style_weight=1e-2, content_weight=1, affinity_weight=1, temporal_weight=1e-1, variation_weight=1e-6, affinity_eye=0):\r\n",
        "    content_loader = DataLoader(content_dataset, 1, shuffle=True)\r\n",
        "    style_loader = iter(DataLoader(style_dataset, content_dataset.batch_size, num_workers=1, sampler=data.RandomSampler(style_dataset, replacement=True, num_samples=epochs * content_dataset.batch_size * len(content_dataset))))\r\n",
        "\r\n",
        "    for it in range(epochs):\r\n",
        "        ts = tss = time()\r\n",
        "        for idx, content_batch in enumerate(content_loader):\r\n",
        "            style_batch = style_loader.__next__().to(device) / 255\r\n",
        "            content_batch = content_batch.squeeze(0).to(device) / 255\r\n",
        "\r\n",
        "            optimizer.zero_grad(True)\r\n",
        "            pred = decoder(encoder(content_batch), encoder(style_batch))\r\n",
        "\r\n",
        "            # print(content_batch.shape, pred.shape)\r\n",
        "            aff_loss = affinity_loss(content_batch[:-1], content_batch[1:], pred[:-1], pred[1:], eye=affinity_eye)\r\n",
        "            content_loss, style_loss = encoder.compute_loss(pred, style_batch)\r\n",
        "            temporal_loss = torch.nn.functional.mse_loss(pred[1:], pred[:-1])\r\n",
        "            variation_loss = total_variation_loss(pred)\r\n",
        "\r\n",
        "            loss = style_weight * style_loss + content_weight * content_loss + affinity_weight * aff_loss + temporal_weight * temporal_loss\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "            if idx % 250 == 0:\r\n",
        "                print(\"{:>3}.{:<5} c_loss {:.4f}  s_loss {:.4f}  a_loss {:.4f}  t_loss {:.4f}  v_loss {:.4f} {:>10.3f}\".format(\r\n",
        "                    it, idx, content_loss.item(), style_loss.item(), aff_loss.item(), temporal_loss.item(), variation_loss.item(), time()-tss))\r\n",
        "                tss = time()\r\n",
        "\r\n",
        "        print(\"{} - {:.3f}s\".format(it, time()-ts))\r\n",
        "        test(encoder, decoder, \"videos/test_{}.mp4\".format(it))\r\n",
        "        state_dict = decoder.model.state_dict()\r\n",
        "        torch.save(state_dict, \"saves/adain_decoder.pth\")\r\n",
        "        state_dict = optimizer.state_dict()\r\n",
        "        torch.save(state_dict, \"saves/adam.pth\")\r\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwkPEb72Gr2j"
      },
      "source": [
        "initialize AdaIN encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_kMP7HAeRaR"
      },
      "source": [
        "encoder = VGGEncoder(\"saves/vgg_normalised.pth\").to(device)\r\n",
        "decoder = AdaIN().to(device)\r\n",
        "save_path = Path(\"saves/adain_decoder.pth\")\r\n",
        "if save_path.is_file():\r\n",
        "    try:\r\n",
        "        state_dict = torch.load(str(save_path))\r\n",
        "        decoder.model.load_state_dict(state_dict)\r\n",
        "        print(\"Loaded previous decoder\", save_path)\r\n",
        "    except:\r\n",
        "        print(\"Error: previous decoder failed to load\")\r\n",
        "        pass\r\n",
        "else:\r\n",
        "    print(\"No previous save. Initializing new decoder\")\r\n",
        "\r\n",
        "optimizer = optim.Adam(decoder.parameters(), lr=1e-6, weight_decay=5e-5)\r\n",
        "save_path = Path(\"saves/adam.pth\")\r\n",
        "if save_path.is_file():\r\n",
        "    try:\r\n",
        "        state_dict = torch.load(str(save_path))\r\n",
        "        optimizer.load_state_dict(state_dict)\r\n",
        "        print(\"Loaded previous optimizer\", save_path)\r\n",
        "    except:\r\n",
        "        print(\"Error: previous optimizer failed to load\")\r\n",
        "        pass\r\n",
        "else:\r\n",
        "    print(\"No previous save. Initializing new optimizer\")\r\n",
        "\r\n",
        "resize = transforms.Resize((256, 256))\r\n",
        "content_dataset = DavisDataset(\"datasets/DAVIS/JPEGImages/480p\", resize)\r\n",
        "style_dataset = ImageFolderDataset(\"datasets/train_1\", resize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xagZZ5ImGo6a"
      },
      "source": [
        "call training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCxfFKwTfnwQ"
      },
      "source": [
        "train(encoder, decoder, optimizer, content_dataset, style_dataset, aff_loss, epochs=100, style_weight=3, affinity_weight=1e5, temporal_weight=10, variation_weight=0, affinity_eye=0.1)\r\n",
        "\r\n",
        "torch.save(decoder.model.state_dict(), \"saves/adain_decoder.pth\")\r\n",
        "torch.save(optimizer.state_dict(), \"saves/adam.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}